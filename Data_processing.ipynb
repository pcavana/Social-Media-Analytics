{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2cf167-9956-4897-a292-249bd31cdc17",
      "metadata": {
        "id": "cf2cf167-9956-4897-a292-249bd31cdc17",
        "outputId": "32beb3c3-d52b-4bb1-fd4f-34eaa1c1e49b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\feder\\anaconda3\\lib\\site-packages (2.1.4)\n",
            "Requirement already satisfied: networkx in c:\\users\\feder\\anaconda3\\lib\\site-packages (3.1)\n",
            "Requirement already satisfied: nltk in c:\\users\\feder\\anaconda3\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: zstandard in c:\\users\\feder\\anaconda3\\lib\\site-packages (0.19.0)\n",
            "Collecting orjson\n",
            "  Obtaining dependency information for orjson from https://files.pythonhosted.org/packages/5d/67/d7837cf0ac956e3c81c67dda3e8f2ffc60dd50ffc480ec7c17f2e22a36ae/orjson-3.9.10-cp311-none-win_amd64.whl.metadata\n",
            "  Downloading orjson-3.9.10-cp311-none-win_amd64.whl.metadata (50 kB)\n",
            "     ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
            "     --------------- ---------------------- 20.5/50.5 kB 640.0 kB/s eta 0:00:01\n",
            "     -------------------------------------- 50.5/50.5 kB 638.6 kB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from pandas) (1.26.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: click in c:\\users\\feder\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\feder\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\feder\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\feder\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\feder\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading orjson-3.9.10-cp311-none-win_amd64.whl (135 kB)\n",
            "   ---------------------------------------- 0.0/135.0 kB ? eta -:--:--\n",
            "   --------------------------- ------------ 92.2/135.0 kB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 135.0/135.0 kB 2.7 MB/s eta 0:00:00\n",
            "Installing collected packages: orjson\n",
            "Successfully installed orjson-3.9.10\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas networkx nltk zstandard orjson"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af5452b2-32e6-49ea-b100-b8604425b843",
      "metadata": {
        "id": "af5452b2-32e6-49ea-b100-b8604425b843"
      },
      "source": [
        "<h1>Importing necessary libraries and downloading the ‘vader_lexicon’ package</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21bba60d-350d-4462-b737-e5b92f51f181",
      "metadata": {
        "id": "21bba60d-350d-4462-b737-e5b92f51f181",
        "outputId": "8fe29839-0900-4d9b-a439-fffcabcc0b0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\feder\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Make sure you have downloaded the 'vader_lexicon' package\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7fb9f2-018c-468d-aaee-9ecd77b9daf9",
      "metadata": {
        "id": "0d7fb9f2-018c-468d-aaee-9ecd77b9daf9"
      },
      "source": [
        "<h1> Reading and cleaning the data</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6254d5d-eaf1-4b13-b7ac-8cf00df89bd1",
      "metadata": {
        "id": "e6254d5d-eaf1-4b13-b7ac-8cf00df89bd1"
      },
      "outputs": [],
      "source": [
        "start_year = 2007\n",
        "end_year = 2022\n",
        "chunksize = 10**6  # Set the chunk size\n",
        "\n",
        "# For each year in the specified range\n",
        "for year in range(start_year, end_year + 1):\n",
        "    # Create the file name\n",
        "    file_name = f\"politics_comments.zst_{year}.csv\"\n",
        "\n",
        "    # Initialize an empty DataFrame for the comments\n",
        "    df_comments = pd.DataFrame()\n",
        "\n",
        "    # Read the file in chunks\n",
        "    for chunk in pd.read_csv(file_name, names=[\"author\",\"body\",\"date\",\"parent_id\",\"id\",\"score\",\"author_fullname\",\"ups\"], chunksize=chunksize):\n",
        "        # Filter rows where 'author' or 'body' is '[deleted]' or '[removed]'\n",
        "        chunk = chunk[~chunk['author'].isin(['[deleted]', '[removed]'])]\n",
        "        chunk = chunk[~chunk['body'].isin(['[deleted]', '[removed]'])]\n",
        "\n",
        "        # Convert the 'date' column to datetime format\n",
        "        chunk['date'] = pd.to_datetime(chunk['date'], unit='s')\n",
        "\n",
        "        # Add the chunk to the DataFrame\n",
        "        df_comments = pd.concat([df_comments, chunk])\n",
        "\n",
        "    # Create a global variable for the DataFrame\n",
        "    globals()[f\"politics_comments_{year}\"] = df_comments\n",
        "\n",
        "    # Create the file name for the submissions\n",
        "    file_name_submissions = f\"politics_submissions.zst_{year}.csv\"\n",
        "\n",
        "    # Initialize an empty DataFrame for the submissions\n",
        "    df_submissions = pd.DataFrame()\n",
        "\n",
        "    # Read the file in chunks\n",
        "    for chunk in pd.read_csv(file_name_submissions, names=[\"author\",\"created_utc\",\"id\",\"num_comments\",\"title\",\"score\",\"author_fullname\",\"ups\"], chunksize=chunksize):\n",
        "        # Filter rows where 'author' is '[deleted]' or '[removed]'\n",
        "        chunk = chunk[~chunk['author'].isin(['[deleted]', '[removed]'])]\n",
        "\n",
        "        # Convert the 'created_utc' column to datetime format\n",
        "        chunk['created_utc'] = pd.to_datetime(chunk['created_utc'], unit='s')\n",
        "\n",
        "        # Add the chunk to the DataFrame\n",
        "        df_submissions = pd.concat([df_submissions, chunk])\n",
        "\n",
        "    # Create a global variable for the DataFrame\n",
        "    globals()[f\"politics_submissions_{year}\"] = df_submissions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d8c723a-d5a0-478b-866b-14e2e2cac7fd",
      "metadata": {
        "id": "8d8c723a-d5a0-478b-866b-14e2e2cac7fd"
      },
      "source": [
        "<h1> Sentiment analysis and splitting the ID</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "844f848b-13cc-4044-bdc9-a587e80336f1",
      "metadata": {
        "id": "844f848b-13cc-4044-bdc9-a587e80336f1"
      },
      "outputs": [],
      "source": [
        "# Create a SentimentIntensityAnalyzer object\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment_scores(comment):\n",
        "    scores = sia.polarity_scores(comment)\n",
        "    return pd.Series(scores)\n",
        "\n",
        "def split_id(fullname):\n",
        "    # Extract the type and ID from the fullname\n",
        "    type_prefix, id_ = fullname.split('_')\n",
        "    # Map the type prefix to a more descriptive string\n",
        "    type_ = 'comment' if type_prefix == 't1' else 'post' if type_prefix == 't3' else 'unknown'\n",
        "    return pd.Series([type_, id_])\n",
        "\n",
        "start_year = 2007\n",
        "end_year = 2022\n",
        "\n",
        "# For each dataframe in that range (inclusive of start and end), the code does a series of operations\n",
        "for year in range(start_year, end_year + 1):\n",
        "    # Get the dataframes for the current year\n",
        "    politics_comments = globals()[f'politics_comments_{year}']\n",
        "    politics_submissions = globals()[f'politics_submissions_{year}']\n",
        "\n",
        "    # Sentiment analysis with VADER\n",
        "    politics_comments['body'] = politics_comments['body'].fillna('').astype(str)\n",
        "    sentiment_scores = politics_comments['body'].apply(get_sentiment_scores)\n",
        "    sentiment_scores.columns = ['negative', 'neutral', 'positive', 'sentiment']\n",
        "    politics_comments = pd.concat([politics_comments, sentiment_scores], axis=1)\n",
        "\n",
        "    # Split fullname\n",
        "    politics_comments[['parent_type', 'parent_id']] = politics_comments['parent_id'].apply(split_id)\n",
        "\n",
        "    # Creation of the corresponding year's connections dataframe\n",
        "    id_author_map_posts = politics_submissions[['id', 'author']].set_index('id').to_dict()['author']\n",
        "    id_author_map_comments = politics_comments[['id', 'author']].set_index('id').to_dict()['author']\n",
        "    connections = politics_comments[['author', 'id', 'parent_id', 'sentiment']].copy()\n",
        "    connections['parent_author'] = politics_comments.apply(lambda row: id_author_map_posts.get(row['parent_id']) if row['parent_type'] == 'post' else id_author_map_comments.get(row['parent_id']), axis=1)\n",
        "\n",
        "    # Save the connections dataframe as a global variable\n",
        "    globals()[f'connections_{year}'] = connections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612b142c-d41d-4474-ae43-b8cbf259bc8a",
      "metadata": {
        "id": "612b142c-d41d-4474-ae43-b8cbf259bc8a"
      },
      "source": [
        "<h1> Saving the dataframes</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "324f2522-f960-40f0-ac04-0648c98aaf09",
      "metadata": {
        "id": "324f2522-f960-40f0-ac04-0648c98aaf09"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def save_politics_dataframes(start_year, end_year):\n",
        "    for year in range(start_year, end_year+1):\n",
        "        for data_type in ['submissions', 'comments']:\n",
        "            df_name = f'politics_{data_type}_{year}'\n",
        "            df = globals()[df_name]\n",
        "            with open(f'{df_name}.pickle', 'wb') as f:\n",
        "                pickle.dump(df, f)\n",
        "\n",
        "def save_connections_dataframes(start_year, end_year):\n",
        "    for year in range(start_year, end_year+1):\n",
        "        df_name = f'connections_{year}'\n",
        "        df = globals()[df_name]\n",
        "        with open(f'{df_name}.pickle', 'wb') as f:\n",
        "            pickle.dump(df, f)\n",
        "\n",
        "# Call the functions\n",
        "save_politics_dataframes(2007, 2022)\n",
        "save_connections_dataframes(2007, 2022)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8495a37-41d4-42f3-b4f9-b0abf867672b",
      "metadata": {
        "id": "c8495a37-41d4-42f3-b4f9-b0abf867672b"
      },
      "source": [
        "<h1>Processing the data and creating the graph</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0747021-c671-474a-ba7e-073ecab620fc",
      "metadata": {
        "id": "d0747021-c671-474a-ba7e-073ecab620fc"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "\n",
        "def process_year(year):\n",
        "    # Name of the pickle file\n",
        "    filename = f'connections_{year}.pickle'\n",
        "\n",
        "    # Open the pickle file and turn it into a dataframe\n",
        "    with open(filename, 'rb') as f:\n",
        "        df = pickle.load(f)\n",
        "\n",
        "    # Create a boolean mask for the rows to keep\n",
        "    mask = (df['parent_author'].notna()) & (df['author'] != df['parent_author'])\n",
        "\n",
        "    # Apply the mask to the DataFrame\n",
        "    df = df.loc[mask]\n",
        "\n",
        "    # Save the modified dataframe with the same name as the original\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(df, f)\n",
        "\n",
        "    # Create a new DataFrame grouped by 'author' and 'parent_author'\n",
        "    df_grouped = df.groupby(['author', 'parent_author'])\n",
        "\n",
        "    # Create a DataFrame with 'number_of_interactions' and 'avg_sentiment'\n",
        "    df_grouped = df_grouped.agg(\n",
        "        number_of_interactions=('author', 'count'),\n",
        "        avg_sentiment=('sentiment', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Create a new column 'interaction_sentiment_weighted'\n",
        "    df_grouped['interaction_sentiment_weighted'] = df_grouped['avg_sentiment'] * df_grouped['number_of_interactions']\n",
        "\n",
        "    # Save the grouped dataframe as a pickle\n",
        "    with open(f'groupby_connections_{year}.pickle', 'wb') as f:\n",
        "        pickle.dump(df_grouped, f)\n",
        "\n",
        "    # Create a full directed graph\n",
        "    G_full = nx.from_pandas_edgelist(df_grouped, 'author', 'parent_author', ['number_of_interactions', 'avg_sentiment'], create_using=nx.DiGraph())\n",
        "\n",
        "    # Save the graph\n",
        "    with open(f'graph_{year}.pkl', 'wb') as f:\n",
        "        pickle.dump(G_full, f)\n",
        "\n",
        "    # Delete the data to save memory\n",
        "    del df\n",
        "    del df_grouped\n",
        "    del G_full\n",
        "    gc.collect()\n",
        "\n",
        "def process_range(start_year, end_year):\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        process_year(year)\n",
        "\n",
        "# Run the process_range function with the desired range of years\n",
        "process_range(2007, 2022)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}