{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook you will find:\n",
        "- SNA Metrics\n",
        "- Community Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1pu6owyrzm93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyvis in c:\\users\\paola\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.3.2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\n",
            "ERROR: No matching distribution found for cv2\n",
            "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\Paola\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvis cv2 gdown spicy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1YG5rJOB17pH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Paola\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import lzma\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import os\n",
        "import networkx as nx\n",
        "import networkx.algorithms.community as nx_comm\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import cv2 as cv\n",
        "from shutil import copyfile\n",
        "import zipfile\n",
        "from pyvis.network import Network\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "from imageio import imread\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "from nrclex import NRCLex\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# For centrality metrics\n",
        "from networkx.algorithms.centrality import (closeness_centrality,\n",
        "                                            betweenness_centrality,\n",
        "                                            degree_centrality)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuRb8Y_FF4Dn"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-aAtl7FOwSKD"
      },
      "outputs": [],
      "source": [
        "import lzma\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "def load_data(graph_names, start_year, end_year):\n",
        "    global_vars = globals()\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for name in graph_names:\n",
        "            with lzma.open(name + \"_\" + str(year) + '.lzma', 'rb') as file:\n",
        "                G = pickle.load(file)\n",
        "                # Numero massimo totale di nodi nel subgrafo\n",
        "                max_total_nodes = 300\n",
        "\n",
        "                # Trova le componenti fortemente connesse\n",
        "                strongly_connected_components = list(nx.strongly_connected_components(G))\n",
        "\n",
        "                # Ordina le componenti in base alla dimensione in modo decrescente\n",
        "                sorted_components = sorted(strongly_connected_components, key=len, reverse=True)\n",
        "\n",
        "                # Crea il subgrafo\n",
        "                subgraph = nx.DiGraph()\n",
        "\n",
        "                for component in sorted_components:\n",
        "                    # Limita il numero di nodi nella componente\n",
        "                    selected_nodes = list(component)\n",
        "                    \n",
        "                    \n",
        "                    if len(subgraph) + len(selected_nodes) <= max_total_nodes:\n",
        "                        # Aggiungi i nodi al subgrafo\n",
        "                        subgraph.add_nodes_from(selected_nodes)\n",
        "                        \n",
        "                        # Aggiungi gli archi al subgrafo\n",
        "                        subgraph.add_edges_from(G.subgraph(selected_nodes).edges())\n",
        "\n",
        "                        print(f\"Nodi nel subgrafo: {len(subgraph)}, Archi nel subgrafo: {subgraph.number_of_edges()}\")\n",
        "                    else:\n",
        "                        break  # Interrompi se l'aggiunta di questa componente supera il limite totale di nodi\n",
        "\n",
        "                if len(subgraph) == 0:\n",
        "                    random_nodes = random.sample(G.nodes(), max_total_nodes)\n",
        "                    subgraph = G.subgraph(random_nodes)\n",
        "                    print(f\"Subgrafo casuale: Nodi nel subgrafo: {len(subgraph)}, Archi nel subgrafo: {subgraph.number_of_edges()}\")\n",
        "\n",
        "                # k = min(50, len(G))  # Scegli il minimo tra 50 e il numero di nodi nel grafo\n",
        "                # nodes = list(G.nodes())\n",
        "                # random_nodes = random.sample(nodes, k)\n",
        "                # H = G.subgraph(random_nodes)  # Crea un sottografo con i nodi campionati casualmente\n",
        "                global_vars[name + \"_\" + str(year)] = subgraph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnYXmUMowwPl"
      },
      "source": [
        "# SNA metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zRGS8f4Bws4m"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def write_to_file(filename, text):\n",
        "    with open(filename, 'a') as f:\n",
        "        f.write(text + '\\n')\n",
        "\n",
        "def get_metrics(G, year):\n",
        "    assortativity = nx.degree_assortativity_coefficient(G)\n",
        "    node_conn = nx.node_connectivity(G)\n",
        "    edge_conn = nx.edge_connectivity(G)\n",
        "    density = nx.density(G)\n",
        "    overall_reciprocity = nx.overall_reciprocity(G)\n",
        "    scc = nx.number_strongly_connected_components(G)\n",
        "    nnodes = G.number_of_nodes()\n",
        "    deg = sum(d for n, d in G.degree) / (2*float(nnodes))\n",
        "\n",
        "    metrics = f'Year: {year}\\nNumber of nodes: {G.number_of_nodes()}\\nNumber of edges: {G.number_of_edges()}\\nAssortativity: {round(assortativity,4)}\\nNode connectivity: {node_conn}\\nEdge connectivity: {edge_conn}\\nGraph density: {round(density,6)}\\nOverall_reciprocity: {round(overall_reciprocity,5)}\\nNumber of strongly connected component: {scc}\\nAverage degree: {deg}\\n'\n",
        "    print(metrics)\n",
        "    write_to_file('SNA_metrics.txt', metrics)\n",
        "\n",
        "def get_degree_centrality(G, year):\n",
        "    dc_dict = dict(nx.degree_centrality(G))\n",
        "    ordered_dc = dict(sorted(dc_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    degree_centrality = '\\nTop 10 degree centrality connected component:'\n",
        "    i = 0\n",
        "    for key, value in ordered_dc.items():\n",
        "        degree_centrality += f'\\n{key}: {value}'\n",
        "        i += 1\n",
        "        if i == 10:\n",
        "            break\n",
        "    print(degree_centrality)\n",
        "    write_to_file('SNA_metrics.txt', degree_centrality)\n",
        "\n",
        "def get_clos_centrality(G, year):\n",
        "    cc_dict = dict(nx.closeness_centrality(G))\n",
        "    ordered_cc = dict(sorted(cc_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    closeness_centrality = '\\nTop 10 closeness centrality connected component:'\n",
        "    i = 0\n",
        "    for key, value in ordered_cc.items():\n",
        "        closeness_centrality += f'\\n{key}: {value}'\n",
        "        i += 1\n",
        "        if i == 10:\n",
        "            break\n",
        "    print(closeness_centrality)\n",
        "    write_to_file('SNA_metrics.txt', closeness_centrality)\n",
        "\n",
        "def get_bet_centrality(G, year):\n",
        "    bc_dict = dict(nx.betweenness_centrality(G))\n",
        "    ordered_bc = dict(sorted(bc_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    betweenness_centrality = '\\nTop 10 betweenness centrality connected component:'\n",
        "    i = 0\n",
        "    for key, value in ordered_bc.items():\n",
        "        betweenness_centrality += f'\\n{key}: {value}'\n",
        "        i += 1\n",
        "        if i == 10:\n",
        "            break\n",
        "    print(betweenness_centrality)\n",
        "    write_to_file('SNA_metrics.txt', betweenness_centrality)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nSVW8jCrGA8U"
      },
      "outputs": [],
      "source": [
        "def SNA_metrics(graph_names, start_year, end_year):\n",
        "    for year in tqdm.tqdm(range(start_year, end_year + 1), desc='Processing years'):\n",
        "        for name in graph_names:\n",
        "            G = globals()[name + \"_\" + str(year)]\n",
        "\n",
        "            get_metrics(G, year)\n",
        "            get_degree_centrality(G, year)\n",
        "            get_clos_centrality(G, year)\n",
        "            get_bet_centrality(G, year)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LppssuaCx1KX"
      },
      "source": [
        "# Community Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RSumOdBnx6S2"
      },
      "outputs": [],
      "source": [
        "def find_communities(G):\n",
        "  c = nx.community.greedy_modularity_communities(G)\n",
        "  print('Numbers of communities:', len(c))\n",
        "  return c\n",
        "\n",
        "def plot_communities(G, c):\n",
        "  partition = {}\n",
        "  for i,comm in enumerate([list(el) for el in c]):\n",
        "    for el in comm:\n",
        "        partition[el] = i\n",
        "\n",
        "  plt.figure(figsize=(15, 15))\n",
        "  plt.axis('off')\n",
        "  nx.draw_networkx(G, pos=nx.spring_layout(G), cmap=plt.cm.viridis, node_color=list(partition.values()), with_labels=False)\n",
        "\n",
        "def get_mod_score(G, c):\n",
        "  print('Modularity score:', nx_comm.modularity(G, c))\n",
        "\n",
        "def get_top_community(c):\n",
        "  partition = {}\n",
        "  for i,comm in enumerate([list(el) for el in c]):\n",
        "      for el in comm:\n",
        "          partition[el] = i\n",
        "\n",
        "  communities = {}\n",
        "  for node, comm_id in partition.items():\n",
        "      communities.setdefault(comm_id, []).append(node)\n",
        "\n",
        "  sorted_communities = {k: v for k, v in sorted(communities.items(), key=lambda item: len(item[1]), reverse=True)}\n",
        "\n",
        "  top_communities = {k: len(v) for k, v in list(sorted_communities.items())[:3]}\n",
        "  print(top_communities)\n",
        "\n",
        "def p_viz(G, communities, year=None):\n",
        "  for c, v_c in enumerate(communities):\n",
        "    if len(v_c) > 2: # coloring only communities with more than 2 users\n",
        "      for v in v_c:\n",
        "        # Add 1 to save 0 for external edges\n",
        "        G.nodes[v]['group'] = c + 1\n",
        "\n",
        "  net = Network(height = '1024px', width = \"100%\", directed=True, notebook=True, cdn_resources='remote')\n",
        "  net.set_options(\"\"\"\n",
        "  var options = {\n",
        "    \"edges\": {\n",
        "      \"color\": {\n",
        "        \"inherit\": true\n",
        "      },\n",
        "      \"smooth\": {\n",
        "        \"type\": \"continuous\",\n",
        "        \"forceDirection\": \"none\",\n",
        "        \"roundness\": 0.65\n",
        "      }\n",
        "    },\n",
        "    \"physics\": {\n",
        "      \"forceAtlas2Based\": {\n",
        "        \"springLength\": 100\n",
        "      },\n",
        "      \"minVelocity\": 0.75,\n",
        "      \"solver\": \"forceAtlas2Based\"\n",
        "    },\n",
        "    \"interaction\": {\n",
        "      \"navigationButtons\": true\n",
        "    }\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "  net.from_nx(G)\n",
        "\n",
        "  html_file_path = f'community_{year}.html' \n",
        "  net.write_html(html_file_path)\n",
        "\n",
        "  net.show('community.html')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UETmXtQ-GHc9"
      },
      "outputs": [],
      "source": [
        "def Community_Detection(graph_names, start_year, end_year):\n",
        "    for year in tqdm.tqdm(range(start_year, end_year + 1), desc='Processing years'):\n",
        "        for name in graph_names:\n",
        "            G = globals()[name + \"_\" + str(year)]\n",
        "\n",
        "            c = find_communities(G)\n",
        "            plot_communities(G, c)\n",
        "            get_mod_score(G, c)\n",
        "            get_top_community(c)\n",
        "            p_viz(G, c, year=year)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_nZbSgp8Fn2N"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def pipeline(graph_names, start_year, end_year):\n",
        "    # Cancella il file esistente per evitare di appendere a vecchi dati\n",
        "    if os.path.exists('SNA_metrics.txt'):\n",
        "        os.remove('SNA_metrics.txt')\n",
        "\n",
        "    for year in tqdm.tqdm(range(start_year, end_year + 1), desc='Processing years'):\n",
        "        load_data(graph_names, year, year)\n",
        "\n",
        "        SNA_metrics(graph_names, year, year)\n",
        "        Community_Detection(graph_names, year, year)\n",
        "\n",
        "        # Cancella i grafici di quell'anno\n",
        "        for name in graph_names:\n",
        "            del globals()[name + \"_\" + str(year)]\n",
        "\n",
        "        # Usa gc per pulire la RAM\n",
        "        gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3KQgJwSFoD5"
      },
      "outputs": [],
      "source": [
        "# Utilizzo della funzione\n",
        "df_names = [\"politics_comments\"]\n",
        "graph_names = [\"graph\"]\n",
        "\n",
        "start_year = 2007\n",
        "end_year = 2009\n",
        "\n",
        "pipeline(graph_names, start_year, end_year)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
